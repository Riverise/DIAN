# DIAN
共包含四个python文件
* utils.py内实现了自定义的accuracy/precision/recall/f1_score等评价指标，以及读取数据集、分批使用数据集的功能函数
* 1.MLP.py内完成第一题MNIST数据集分类任务
* 2.RNN.py内完成第二题FashionMNIST数据集分类任务
* 3.Attention.py内完成了第三题MHA/MQA/GQA的实现

# 学习记录：
## 第一题
运行程序即可得到不同条件下（网络层数、神经元个数、训练步数）的实验结果，实验结果也写在文件末尾的注释中了
此前都是看别人的代码比较多，但自己实践还是能收获不少。
比如data_iter的功能本质上是由python自带的关键字yield实现的。通过yield，可以实现每次迭代时，函数返回特定数量的样本，对torch之类的库封装的data_iter有了更深入的理解。
还学习了如何做深度学习的实验，即通过字典与列表设置一系列configuration，然后通过**解包作为实例化自定义网络的参数。
模型调优方面挺顺利，按照题目给定的隐藏层设置很快就让准确率到90%以上。

## 第二题
以前接触过RNN但理解不是很深，在看了相关讲解之后明白它的数学原理了。在实现过程中发现这是一个多对一问题，即一个序列对应一个标签，但其实区别也不大，只需将输出从每个时间步输出一个改为只有最后一步输出，按照数学原理使用nn.Linear()实现SimpleRNN难度不大。

主要遇到的问题是模型效果的优化，最开始跑的时候（隐藏层维度只有56，学习率0.001,batch_size为2000/1000），结果batch_loss从第一个epoch开始就一直降不下来，都是2.x的样子（准确率为20%+），就怀疑是网络参数没有被加入到SGD优化器里面去或者梯度太小（梯度消失）导致网络参数不更新，于是就开了断点调试，就发现SGD要优化的参数是没问题的，但是在loss.backward()步骤后网络参数的grad很小，1e-3/1e-4的数量级，我就调高学习率，先调到0.05，又调到0.1，发现这个过程模型评估出的准确率慢慢升高到47%，再往上调就没效果了。

后来又查询了一系列资料，最后做了如下改进：
1. 增加隐藏层维度（神经元个数），从56提高到128再到256，模型准确率从47%到53%再到59%
2. 将神经网络最后的softmax层换成Logsoftmax层，模型准确率从59%提高到64%
3. 在输入给神经网络前对输入数据集进行归一化（[0, 255]除以255即可归一化为[0, 1]）,准确率从64%到81%，帮助很大
4. 将epoch数从10改到15，准确率提高到82.6%
5. 在权重初始化时使用orthogonal初始化，提高到84.6%。

还尝试过更换激活函数，发现Relu与sigmoid效果都挺差的,sigmoid最差，训练过程中loss几乎不变。
后来得知使用nn.CrossEntropyLoss作为损失函数时，模型在输出前不需要softmax/logsoftmax，因为它会在计算交叉熵时自动实现。

## 第三题
技术测试前刚接触过attention机制，只是从原理上理解，但没有自己实现过。因此着手时先看了Attention Is All You Need那篇论文，画出了MultiHeadAttention机制中矩阵变换的流程图，作为编程的蓝图。个人认为实现过程中张量的维度设置是关键，因此写的时候格外小心，写的过程也注释了维度的变化。论文中都是二维矩阵，但是实际中还有第三维度batch_size，因此就涉及到三维矩阵的乘法，在查阅相关资料后就明白三维矩阵乘法的机制了，于是就顺利写出了多头注意力机制的代码。

MQA/GQA的概念之前都没接触过，看pdf里的图乍一看不知道是什么意思，但是在阅读一篇讲解之后，就明白是什么意思了，其实就是在多头注意力机制的基础上让多个或全部query共享一个Key、value向量，在一定程度上减少了要学习的参数量，却也有不错的效果。于是在MHA的基础上稍作修改，就实现了MQA/GQA，整体比较顺利。

